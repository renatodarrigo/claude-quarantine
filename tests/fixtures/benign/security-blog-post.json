{
  "tool_name": "WebFetch",
  "tool_result": {
    "content": "# Understanding Prompt Injection Attacks\n\nPrompt injection is a class of attacks against LLM-based applications where adversarial text attempts to override the system prompt or manipulate the model's behavior.\n\n## Common Attack Patterns\n\nAttackers typically use phrases that:\n- Instruct the model to disregard its original purpose\n- Attempt role reassignment (e.g., telling the AI it has a \"new role\")\n- Impersonate system-level messages using XML-like tags\n- Direct the model to perform unauthorized actions\n\n## Simon Willison's Rule of Two\n\nSimon Willison proposes that an agent can safely have at most 2 of 3 properties:\n1. Access to private data\n2. Exposure to untrusted content  \n3. Ability to take state-changing actions\n\nHaving all three simultaneously creates an inherently risky configuration.\n\n## Defenses\n\nEffective defenses include:\n- Input sanitization and pattern matching\n- LLM-based semantic analysis\n- Sandboxing and least-privilege execution\n- Content provenance tracking\n\nNo single defense is foolproof â€” defense in depth is the recommended approach.\n\n## Research References\n\n- Perez & Ribeiro (2022): \"Prompt Injection attack against GPT-3\"\n- Greshake et al. (2023): \"Not what you've signed up for\"\n- OWASP Top 10 for LLM Applications\n\nResponsible disclosure practices should always be followed when reporting injection vulnerabilities."
  }
}
